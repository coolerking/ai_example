{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題\n",
    "\n",
    "- 6-armed Bandits\n",
    "\n",
    "# アルゴリズム\n",
    "\n",
    "- 方策勾配\n",
    "\n",
    "## 損失関数\n",
    "\n",
    "\\begin{equation*}\n",
    "Loss = - \\log \\pi A\n",
    "\\end{equation*}\n",
    "\n",
    "Π : アクションのウェイト\n",
    "A : アドバンテージ(報酬)\n",
    "\n",
    "\n",
    "# 実行環境\n",
    "\n",
    "- CPU: Core i7 @2.80GHz\n",
    "- Mem: 16GB\n",
    "- HDD: 4TB\n",
    "- GPU: GeForce1050Ti 4GB\n",
    "- OS: Ubuntu16.04LTS\n",
    "- Con: Docker CE/tensorflow1.4.0-gpu/Python3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 問題\n",
    "\n",
    "# アームの閾値 値が低いほどあたりが出やすい\n",
    "bandits = [0.2, -0.5, -0.2, 0, -0.1, 0.2]\n",
    "\n",
    "# アームの本数\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "# アームを引くと結果(1か-1)を返す\n",
    "def pullBandit(bandit):\n",
    "    '''\n",
    "    アームの閾値を渡すとあたり(1)かはずれ(-1)かを返す\n",
    "    '''\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 計算グラフ\n",
    "\n",
    "# 計算グラフをリセット\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 重み\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "\n",
    "# 何番目のアームを引くか\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# 報酬を格納するための変数\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "\n",
    "# アクションを保持するための変数\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "\n",
    "# アクションのウェイトを取り出す変数　重みから現在選択しているアクションに入っているウェイトを取り出す\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "\n",
    "# 損失関数\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "\n",
    "# 最適化\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "# モデル更新\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward list: [-1.  0.  0.  0.  0.  0.]\n",
      "Reward list: [ -1.  23.   0.   0.   1.   0.]\n",
      "Reward list: [ -1.  53.   0.   0.   2.  -1.]\n",
      "Reward list: [ -1.  78.   3.   0.   2.  -1.]\n",
      "Reward list: [ -1.  86.   3.   0.   3.  -2.]\n",
      "Reward list: [  -2.  102.    2.    2.    3.   -2.]\n",
      "Reward list: [  -2.  118.    1.    2.    3.   -1.]\n",
      "Reward list: [  -3.  127.    1.    2.    5.   -1.]\n",
      "Reward list: [  -2.  151.    0.    2.    4.   -2.]\n",
      "Reward list: [  -1.  172.   -2.    1.    5.   -2.]\n",
      "Reward list: [  -1.  181.   -1.    2.    5.   -3.]\n",
      "Reward list: [  -3.  195.   -1.    0.    5.   -3.]\n",
      "Reward list: [  -2.  200.   -4.    0.    6.   -3.]\n",
      "Reward list: [  -2.  204.   -4.    0.    6.   -3.]\n",
      "Reward list: [  -3.  209.   -3.    1.    6.   -5.]\n",
      "Reward list: [  -3.  219.   -4.    1.    3.   -5.]\n",
      "Reward list: [   0.  230.   -5.    1.    4.   -5.]\n",
      "Reward list: [   1.  250.   -7.    0.    4.   -5.]\n",
      "Reward list: [   1.  288.   -7.    0.    4.   -5.]\n",
      "Reward list: [   1.  304.   -7.    0.    4.   -5.]\n",
      "Best arc on agent is 2 st/nd/rd/th arm!\n"
     ]
    }
   ],
   "source": [
    "# トレーニング\n",
    "\n",
    "## パラメータ\n",
    "# 試行回数\n",
    "total_episodes = 1000\n",
    "# 累積報酬\n",
    "total_reward = np.zeros(num_bandits)\n",
    "# 閾値 ε\n",
    "e = 0.1\n",
    "\n",
    "# TensorFlow 変数の初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 変数の初期化\n",
    "    sess.run(init)\n",
    "    # カウンタ\n",
    "    i = 0;\n",
    "    while i < total_episodes:\n",
    "        # 閾値より乱数が小さい場合\n",
    "        if np.random.rand(1) < e:\n",
    "            # ランダムにアクションを決定する\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            #print(\"chosen_action\")\n",
    "            action = sess.run(chosen_action)\n",
    "        # 報酬を計算\n",
    "        #print(action)\n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        # モデルを更新、ウェイトの取り出し\n",
    "        _,resp,ww = sess.run([update, responsible_weight, weights], feed_dict={\n",
    "            reward_holder: [reward], \n",
    "            action_holder: [action]})\n",
    "        \n",
    "        # 累積報酬\n",
    "        total_reward[action] += reward\n",
    "        # \n",
    "        if i % 50 == 0:\n",
    "            print(\"Reward list: \" + str(total_reward))\n",
    "        i += 1\n",
    "\n",
    "print(\"Best arc on agent is \" + str(np.argmax(ww) + 1) + \" st/nd/rd/th arm!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
