{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym\n",
    "\n",
    "- [OpenAI Gym](https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "## インストール\n",
    "\n",
    "- [GitHub: OpenAI Gym](https://github.com/openai/gym)\n",
    "\n",
    "```\n",
    "pip install gym\n",
    "```\n",
    "\n",
    "## FrozenLake\n",
    "\n",
    "凍った湖の上をあるき、スタートからゴールまでホールに落ちないように最後まで到達すると成功するゲーム\n",
    "\n",
    "- マス目：ここでは4x4とする\n",
    "- 各マス目\n",
    "\n",
    " - S:Start 開始位置\n",
    " - F: Frozren Staff 落ちない\n",
    " - H: Hole 落ちる\n",
    " - G: Goal 目的地点 報酬1\n",
    "\n",
    "氷の上は滑ったり、風が吹いたりするので、必ずしもアクションで指定した方向に着地するとは限らない、という前提でデータが作成されている\n",
    "\n",
    "# Qテーブル\n",
    "\n",
    "- 方策（上下左右）×位置（０から１５）\n",
    "- それぞれのQ値（価値）を格納\n",
    " - 価値：報酬の累積和、最初はゼロで初期化\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "# ベルマン方程式\n",
    "\n",
    "\\begin{equation*}\n",
    "Q(s,a) = r + γ(max(Q(s', a'))\n",
    "\\end{equation*}\n",
    "\n",
    "r: 報酬\n",
    "s:state\n",
    "a:action\n",
    "\n",
    "s':次のstate\n",
    "a':次のaction\n",
    "\n",
    "Q(s,a): 行動価値観数\n",
    "γ：割引率\n",
    "\n",
    "max..最大値を与える組み合わせを求める手法は様々\n",
    "\n",
    "- グリーディー法:今までうまく言った方法をひたすらうつ\n",
    "- ランダム法\n",
    "- ε-グリーディー法：時々ランダムにうつが、良いスコアを出るほうを採用する\n",
    "\n",
    "# Q値の更新方法\n",
    "\n",
    "- TD-Q学習 (Temporal Difference)\n",
    "その状態と次の状態を求めて次々と更新する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習レート\n",
    "lr = 0.8\n",
    "# 割引率\n",
    "y = 0.95\n",
    "# 何回試行するか\n",
    "num_episodes = 2000\n",
    "# 報酬一覧を格納する配列\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_episodes 回まわす\n",
    "for i in range(num_episodes):\n",
    "    # 状態の初期化\n",
    "    s = env.reset()\n",
    "    # 報酬の累積和\n",
    "    rAll = 0\n",
    "    # Holeに落ちたらdieのd\n",
    "    d = False\n",
    "    # 1試行内のカウンタ\n",
    "    j = 0\n",
    "    # \n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        # グリーディー法\n",
    "        # Q[s,:] あるstateの時のactionごとのQ値を格納する配列\n",
    "        # np.random.randn(1, env.action_space.n) * ( 1.0 / (i + 1) ) ノイズ\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n) * ( 1.0 / (i + 1) ) ) \n",
    "        # アクションの実行結果を取得する\n",
    "        s1, r, d, _ = env.step(a)\n",
    "        # ベルマン方程式を更新する、その際学習率をかけてちょっとだけ学習させる\n",
    "        Q[s,a] = Q[s,a] + lr * (r + y*np.max(Q[s1,:]) - Q[s, a])\n",
    "        # 報酬の総和：今回の報酬rを足す\n",
    "        rAll += r\n",
    "        # 状態を更新\n",
    "        s = s1\n",
    "        # Hole に落ちていないなら次のstep()を実行する\n",
    "        if d == True:\n",
    "            break\n",
    "    # 報酬一覧に今回の結果を追加する\n",
    "    rList.append(rAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回数ごとの結果: 0.6695\n"
     ]
    }
   ],
   "source": [
    "print(\"回数ごとの結果:\", str( sum(rList)/num_episodes) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最終的なQテーブルの値\n",
      "[[  2.48293691e-01   8.23027540e-03   9.99690364e-03   1.03237589e-02]\n",
      " [  4.42217040e-04   2.48921589e-03   2.25388633e-03   1.52061875e-01]\n",
      " [  4.57943510e-03   1.38529495e-01   3.01624101e-03   5.31811925e-03]\n",
      " [  2.84005368e-03   1.29686283e-03   2.99853322e-04   7.47721621e-02]\n",
      " [  3.72997747e-01   2.58546603e-03   5.59605533e-04   9.74355882e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.32062627e-04   1.86793485e-04   2.27938152e-03   3.25088509e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   8.11013929e-03   3.77990628e-01]\n",
      " [  6.07385150e-03   2.27305254e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  4.76323205e-02   0.00000000e+00   1.05688196e-03   1.15378573e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   3.74654272e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   8.89083028e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(\"最終的なQテーブルの値\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
