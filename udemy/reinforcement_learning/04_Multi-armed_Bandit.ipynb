{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方策勾配 Polycy Gradient\n",
    "\n",
    "\n",
    "- アクションがどれくらい効果的だったのか？\n",
    "\n",
    " - 試行ごとに計算、**アクションを選択する方針（方策・ポリシー）** を最適化\n",
    "\n",
    "- どの手が有効か？の選択基準を **教師なし学習** で学ぶ\n",
    "\n",
    "# Multi-armed Bandit Problem\n",
    "\n",
    "- 方策勾配のサンプルとしてよく使われるのが、スロットマシン(Bandit)\n",
    "- スロットマシンに複数のアームがついている\n",
    "- どのアームをどの順番で\n",
    "\n",
    "# 適用対象\n",
    "\n",
    "限られたリソースをいかに最適配分するか？\n",
    "\n",
    "- ギャンブル\n",
    "- 研究開発投資\n",
    "- 臨床試験\n",
    "- コンテンツ管理\n",
    "\n",
    "# 4-Bandits Problem\n",
    "\n",
    "4本腕のバンディット。\n",
    "\n",
    "- 各アームごとにあたる閾値が異なる\n",
    "\n",
    "|0.2|-0.5|-0.2|0|\n",
    "\n",
    "- 閾値が低いほど当たりやすい\n",
    "- 0から1の乱数を発生させて都度判定\n",
    "\n",
    "1. アームを引く\n",
    "2. 報酬を得る（アタリ、はずれ）\n",
    "3. 損失関数を計算する\n",
    "4. 最小化するようWを更新\n",
    "5. 1へ戻る\n",
    "\n",
    "## 損失関数\n",
    "\n",
    "\\begin{equation*}\n",
    "Loss = - \\log \\pi A\n",
    "\\end{equation*}\n",
    "\n",
    "Π : アクションのウェイト\n",
    "A : アドバンテージ(報酬)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アームの閾値 値が低いほどあたりが出やすい\n",
    "bandits = [0.2, -0.5, -0.2, 0]\n",
    "\n",
    "# アームの本数\n",
    "num_bandits = len(bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アームの閾値を渡すとあたり(1)かはずれ(-1)かを返す\n",
    "def pullBandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 計算グラフ\n",
    "\n",
    "# 計算グラフをリセット\n",
    "tf.reset_default_graph\n",
    "\n",
    "# 重み\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "\n",
    "# 何番目のアームを引くか\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "# 報酬を格納するための変数\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "\n",
    "# アクションを保持するための変数\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "\n",
    "# アクションのウェイトを取り出す変数　重みから現在選択しているアクションに入っているウェイトを取り出す\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "\n",
    "# 損失関数\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "\n",
    "# 最適化\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "# モデル更新\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward list: [-1.  0.  0.  0.]\n",
      "Reward list: [ -3.  19.  -2.  -1.]\n",
      "Reward list: [ -5.  40.  -3.  -1.]\n",
      "Reward list: [ -5.  54.  -3.  -1.]\n",
      "Reward list: [ -5.  69.  -3.   0.]\n",
      "Reward list: [ -5.  74.  -2.   0.]\n",
      "Reward list: [ -4.  85.  -1.   1.]\n",
      "Reward list: [  -4.  111.   -1.    1.]\n",
      "Reward list: [  -4.  138.    0.    1.]\n",
      "Reward list: [  -5.  162.   -1.    1.]\n",
      "Reward list: [  -8.  184.   -2.    1.]\n",
      "Reward list: [  -8.  213.   -3.    3.]\n",
      "Reward list: [  -6.  231.   -2.    2.]\n",
      "Reward list: [  -7.  244.   -2.    2.]\n",
      "Reward list: [  -9.  269.   -3.    2.]\n",
      "Reward list: [ -10.  299.   -3.    1.]\n",
      "Reward list: [ -10.  303.   -5.    1.]\n",
      "Reward list: [ -11.  323.   -4.    3.]\n",
      "Reward list: [ -12.  338.   -2.    3.]\n",
      "Reward list: [ -12.  354.   -2.    3.]\n",
      "Best arc on agent is 2th arm!\n"
     ]
    }
   ],
   "source": [
    "# トレーニング\n",
    "\n",
    "## パラメータ\n",
    "# 試行回数\n",
    "total_episodes = 1000\n",
    "# 累積報酬\n",
    "total_reward = np.zeros(num_bandits)\n",
    "# 閾値 ε\n",
    "e = 0.1\n",
    "\n",
    "# TensorFlow 変数の初期化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 変数の初期化\n",
    "    sess.run(init)\n",
    "    # カウンタ\n",
    "    i = 0;\n",
    "    while i < total_episodes:\n",
    "        # 閾値より乱数が小さい場合\n",
    "        if np.random.rand(1) < e:\n",
    "            # ランダムにアクションを決定する\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(chosen_action)\n",
    "        # 報酬を計算\n",
    "        reward = pullBandit(bandits[action])\n",
    "        \n",
    "        # モデルを更新、ウェイトの取り出し\n",
    "        _,resp,ww = sess.run([update, responsible_weight, weights], feed_dict={\n",
    "            reward_holder: [reward], \n",
    "            action_holder: [action]})\n",
    "        \n",
    "        # 累積報酬\n",
    "        total_reward[action] += reward\n",
    "        # \n",
    "        if i % 50 == 0:\n",
    "            print(\"Reward list: \" + str(total_reward))\n",
    "        i += 1\n",
    "\n",
    "print(\"Best arc on agent is \" + str(np.argmax(ww) + 1) + \"th arm!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
